{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c6bd0f-bac1-4640-b670-8166a69da32d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823993c-d308-4f1e-a912-185b86f65265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln, logsumexp\n",
    "from typing import Union, NoReturn, Tuple, List\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dcdc3-9124-47c2-8aba-93b75234dfc8",
   "metadata": {},
   "source": [
    "# Load artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc2dfd-8fd0-45e3-960b-4056dce93c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input: str = 'outputs/preprocess'\n",
    "x_wordtype_counts_left: np.ndarray = joblib.load(f'{path_input}/x_wordtype_counts_left.joblib')\n",
    "x_wordtype_counts_right: np.ndarray = joblib.load(f'{path_input}/x_wordtype_counts_right.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25cc05-f3e3-40a3-8ec1-17006490cc31",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f2718-fd8a-4795-8e38-9edcddddb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GibbsSampler:\n",
    "    \"\"\"GibbsSampler.\"\"\"\n",
    "\n",
    "    def __init__(self, x_wordtype_counts_left: np.ndarray, x_wordtype_counts_right: np.ndarray, n_classes: int):\n",
    "        \"\"\"Init.\"\"\"\n",
    "        # validate input params\n",
    "        assert isinstance(x_wordtype_counts_left, np.ndarray)\n",
    "        assert isinstance(x_wordtype_counts_right, np.ndarray)\n",
    "        assert isinstance(n_classes, int)\n",
    "        assert n_classes > 0\n",
    "\n",
    "        self._x_wordtype_counts_left = x_wordtype_counts_left\n",
    "        self._x_wordtype_counts_right = x_wordtype_counts_right\n",
    "        self._n_classes = n_classes\n",
    "\n",
    "        # initialise gibbs structures\n",
    "        self._x_class_priors = np.array([1 / self._n_classes] * self._n_classes) # shape: Z (n_classes,)\n",
    "        self._n_wordtypes, self._n_features = self._x_wordtype_counts_left.shape\n",
    "        self._x_wordtype_class_assignments = np.random.choice(range(len(self._x_class_priors)), p=self._x_class_priors, size=self._n_wordtypes) # shape: M (n_wordtypes,)\n",
    "        self._x_class_counts = np.bincount(self._x_wordtype_class_assignments) # shape: Z (n_classes,)\n",
    "        self._x_class_wordtype_counts_left = np.zeros((self._n_classes, self._n_features)).astype(int)\n",
    "        self._x_class_wordtype_counts_right = np.zeros((self._n_classes, self._n_features)).astype(int)\n",
    "        np.add.at(self._x_class_wordtype_counts_left, self._x_wordtype_class_assignments, self._x_wordtype_counts_left)\n",
    "        np.add.at(self._x_class_wordtype_counts_right, self._x_wordtype_class_assignments, self._x_wordtype_counts_right)\n",
    "        self._x_class_wordtype_counts_sum_left = self._x_class_wordtype_counts_left.sum(axis=1)\n",
    "        self._x_class_wordtype_counts_sum_right = self._x_class_wordtype_counts_right.sum(axis=1)\n",
    "        self._x_wordtype_counts_sum_left = self._x_wordtype_counts_left.sum(axis=1)  # M (n_wordtypes)\n",
    "        self._x_wordtype_counts_sum_right = self._x_wordtype_counts_right.sum(axis=1)  # M (n_wordtypes)\n",
    "\n",
    "        # validate gibbs structures\n",
    "        _ = self._validate_initialisation()\n",
    "\n",
    "    def _validate_initialisation(self) -> Union[None, NoReturn]:\n",
    "        \"\"\"Validate initialisation structures.\"\"\"\n",
    "        assert self._x_class_priors.shape == (self._n_classes,)\n",
    "        assert self._x_wordtype_class_assignments.shape == (self._n_wordtypes,)\n",
    "        assert self._x_class_counts.shape == (self._n_classes,)\n",
    "        assert self._x_wordtype_class_assignments.shape == (self._n_wordtypes,)\n",
    "        assert self._x_class_wordtype_counts_left.shape == (self._n_classes, self._n_features)\n",
    "        assert self._x_class_wordtype_counts_right.shape == (self._n_classes, self._n_features)\n",
    "        assert self._x_class_wordtype_counts_sum_left.shape == (self._n_classes,)\n",
    "        assert self._x_class_wordtype_counts_sum_right.shape == (self._n_classes,)\n",
    "        assert self._x_wordtype_counts_sum_left.shape == (self._n_wordtypes,)\n",
    "        assert self._x_wordtype_counts_sum_right.shape == (self._n_wordtypes,)\n",
    "    \n",
    "    def _compute_log_conditional_probability(self, ix: int, alpha: float, beta_left: float, beta_right: float) -> np.ndarray:\n",
    "        \"\"\"Compute Gibbs sampling log conditional.\"\"\"\n",
    "        # compute prior\n",
    "        log_probs: float = np.log(self._x_class_counts + alpha) # drop denominator since its common to all classes.\n",
    "\n",
    "        # left context features\n",
    "        log_probs += (\n",
    "            (\n",
    "                gammaln(self._x_class_wordtype_counts_left + self._x_wordtype_counts_left[ix] + beta_left)\n",
    "                - gammaln(self._x_class_wordtype_counts_left + beta_left)\n",
    "            ).sum(axis=1)\n",
    "            - (\n",
    "                gammaln(self._x_class_wordtype_counts_sum_left + self._x_wordtype_counts_sum_left[ix] + self._n_features * beta_left)\n",
    "                - gammaln(self._x_class_wordtype_counts_sum_left + self._n_features * beta_left)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # right context features\n",
    "        log_probs += (\n",
    "            (\n",
    "                gammaln(self._x_class_wordtype_counts_right + self._x_wordtype_counts_right[ix] + beta_right)\n",
    "                - gammaln(self._x_class_wordtype_counts_right + beta_right)\n",
    "            ).sum(axis=1)\n",
    "            - (\n",
    "                gammaln(self._x_class_wordtype_counts_sum_right + self._x_wordtype_counts_sum_right[ix] + self._n_features * beta_right)\n",
    "                - gammaln(self._x_class_wordtype_counts_sum_right + self._n_features * beta_right)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # normalise\n",
    "        log_probs -= logsumexp(log_probs)\n",
    "        \n",
    "        # convert to probs\n",
    "        probs: float = np.exp(log_probs)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def _compute_log_joint_probability(self, alpha: float, beta_left: float, beta_right: float) -> float:\n",
    "        \"\"\"Compute collapsed log joint: log p(z, f | alpha, beta).\"\"\"\n",
    "\n",
    "        # Prior over z: Dirichlet-multinomial on class counts\n",
    "        log_prob: float = gammaln(self._n_classes * alpha) - gammaln(self._n_wordtypes + self._n_classes * alpha)\n",
    "        log_prob += (gammaln(self._x_class_counts + alpha) - gammaln(alpha)).sum()\n",
    "\n",
    "        # Likelihood: product over classes of Dirichlet-multinomial on feature counts\n",
    "        # left context\n",
    "        log_prob += (gammaln(self._n_features * beta_left) - gammaln(self._x_class_wordtype_counts_sum_left + self._n_features * beta_left)).sum()\n",
    "        log_prob += (gammaln(self._x_class_wordtype_counts_left + beta_left) - gammaln(beta_left)).sum()\n",
    "        # right context\n",
    "        log_prob += (gammaln(self._n_features * beta_right) - gammaln(self._x_class_wordtype_counts_sum_right + self._n_features * beta_right)).sum()\n",
    "        log_prob += (gammaln(self._x_class_wordtype_counts_right + beta_right) - gammaln(beta_right)).sum()\n",
    "\n",
    "        return log_prob\n",
    "        \n",
    "\n",
    "    def _remove_class_assignment(self, ix: int) -> int:\n",
    "        \"\"\"Remove word type class assignment.\"\"\"\n",
    "        assert isinstance(ix, (int, np.integer))\n",
    "        \n",
    "        # get word type class assignment\n",
    "        z: int = self._x_wordtype_class_assignments[ix]\n",
    "        \n",
    "        # decrement class count\n",
    "        self._x_class_counts[z] -= 1\n",
    "        \n",
    "        # decrement class word type counts\n",
    "        self._x_class_wordtype_counts_left[z] -= self._x_wordtype_counts_left[ix]\n",
    "        self._x_class_wordtype_counts_right[z] -= self._x_wordtype_counts_right[ix]\n",
    "        \n",
    "        # decrement from class totals\n",
    "        self._x_class_wordtype_counts_sum_left[z] -= self._x_wordtype_counts_sum_left[ix]\n",
    "        self._x_class_wordtype_counts_sum_right[z] -= self._x_wordtype_counts_sum_right[ix]\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _add_class_assignment(self, ix: int, z: int) -> None:\n",
    "        \"\"\"Add word type class assignment.\"\"\"\n",
    "        assert isinstance(ix, (int, np.integer))\n",
    "        assert isinstance(z, (int, np.integer))\n",
    "        \n",
    "        # assign new class\n",
    "        self._x_wordtype_class_assignments[ix] = z\n",
    "        \n",
    "        # increment class count\n",
    "        self._x_class_counts[z] += 1\n",
    "        \n",
    "        # increment class word type counts\n",
    "        self._x_class_wordtype_counts_left[z] += self._x_wordtype_counts_left[ix]\n",
    "        self._x_class_wordtype_counts_right[z] += self._x_wordtype_counts_right[ix]\n",
    "        \n",
    "        # increment from class totals\n",
    "        self._x_class_wordtype_counts_sum_left[z] += self._x_wordtype_counts_sum_left[ix]\n",
    "        self._x_class_wordtype_counts_sum_right[z] += self._x_wordtype_counts_sum_right[ix]\n",
    "        \n",
    "    def _run_sweep(self, alpha: float, beta_left: float, beta_right: float) -> None:\n",
    "        \"\"\"Run Gibbs sweep.\"\"\"\n",
    "        # --- gibbs sweep\n",
    "        for ix_wordtype in np.random.permutation(self._n_wordtypes):\n",
    "    \n",
    "            # --- remove word type assignment\n",
    "            z_old: int = self._remove_class_assignment(ix=ix_wordtype)\n",
    "            \n",
    "            # --- recompute gibbs log conditional\n",
    "            class_probs: np.ndarray = self._compute_log_conditional_probability(ix=ix_wordtype, alpha=alpha, beta_left=beta_left, beta_right=beta_right)\n",
    "\n",
    "            # --- sample new assignment\n",
    "            z_new: int = np.random.choice(self._n_classes, p=class_probs)\n",
    "\n",
    "            # --- add word type assignment\n",
    "            _ = self._add_class_assignment(ix=ix_wordtype, z=z_new)\n",
    "\n",
    "    def compute_posterior_class_probs(self, samples, wordtype_index) -> np.ndarray:\n",
    "        \"\"\"Compute empirical posterior P(z_j = c | data) for one word type.\"\"\"\n",
    "        x_counts: np.ndarray = np.zeros(self._n_classes, dtype=float)\n",
    "        for z in samples:\n",
    "            x_counts[z[wordtype_index]] += 1\n",
    "        probs: np.ndarray = x_counts / x_counts.sum()\n",
    "        return probs\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_word_type_posterior_entropy(x_word_type_posterior_probs: np.ndarray, epsilon: float = 1e-12):\n",
    "        \"\"\"Compute word type posterior entropy.\"\"\"\n",
    "        x_word_type_posterior_probs_clipped = np.clip(x_word_type_posterior_probs, epsilon, 1.0)\n",
    "        return -(x_word_type_posterior_probs_clipped * np.log(x_word_type_posterior_probs_clipped)).sum(axis=1)\n",
    "\n",
    "    def run(self, n_iterations: int, alpha: float, beta_left: float, beta_right: float, n_burn_in: int, n_thinning: int) -> Tuple[List[float], List[np.ndarray], List[int], np.ndarray]:\n",
    "        \"\"\"Run Gibbs sampler.\"\"\"\n",
    "        assert isinstance(n_iterations, int)\n",
    "        assert n_iterations > 0\n",
    "        assert isinstance(alpha, float)\n",
    "        assert isinstance(beta_left, float)\n",
    "        assert isinstance(beta_right, float)\n",
    "\n",
    "        log_probs_trace, class_counts_trace, samples = [], [], []\n",
    "        # reset posterior accumulators\n",
    "        x_word_type_posterior_counts: np.ndarray = np.zeros((self._n_wordtypes, self._n_classes), dtype=int)\n",
    "        n_posterior_samples_kept: int = 0\n",
    "        \n",
    "        for ix_iteration in range(n_iterations):\n",
    "            _ = self._run_sweep(alpha=alpha, beta_left=beta_left, beta_right=beta_right)\n",
    "            log_prob: float = self._compute_log_joint_probability(alpha=alpha, beta_left=beta_left, beta_right=beta_right)\n",
    "\n",
    "            # trace\n",
    "            log_probs_trace.append(log_prob)\n",
    "            class_counts_trace.append(self._x_class_counts.copy())\n",
    "\n",
    "            # collect thinned samples after burn-in\n",
    "            if ix_iteration >= n_burn_in and ((ix_iteration - n_burn_in) % n_thinning == 0):\n",
    "                samples.append(self._x_wordtype_class_assignments.copy())\n",
    "                \n",
    "                # online posterior accumulation: increment (j, z_j)\n",
    "                x_word_type_posterior_counts[np.arange(self._n_wordtypes), self._x_wordtype_class_assignments] += 1\n",
    "                n_posterior_samples_kept += 1\n",
    "\n",
    "            # log\n",
    "            print(f'Iteration: {ix_iteration} log_prob: {log_prob}')\n",
    "\n",
    "        x_word_type_posterior_probs: np.ndarray = x_word_type_posterior_counts / float(n_posterior_samples_kept)\n",
    "\n",
    "        return log_probs_trace, class_counts_trace, samples, x_word_type_posterior_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae689f-0f97-4a0f-9287-da5f94524d5f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5310ab-4b40-44d1-ae4e-9887a60d096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_iterations = 300\n",
    "n_burn_in = 100\n",
    "n_thinning = 5\n",
    "alpha = .1\n",
    "beta_left, beta_right = .5, .5\n",
    "\n",
    "sampler = GibbsSampler(x_wordtype_counts_left=x_wordtype_counts_left, x_wordtype_counts_right=x_wordtype_counts_right, n_classes=n_classes)\n",
    "\n",
    "log_probs_trace, class_counts_trace, samples, x_word_type_posterior_probs = sampler.run(\n",
    "    n_iterations=n_iterations, alpha=alpha, beta_left=beta_left, beta_right=beta_right,\n",
    "    n_burn_in=n_burn_in, n_thinning=n_thinning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f101d-5b29-4e4f-a7b7-09ef3fe1b4fb",
   "metadata": {},
   "source": [
    "# Dump artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e05be-82b5-4185-a418-cbc4267f1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = f'outputs/train/alpha={alpha}_beta_left={beta_left}_beta_right={beta_right}'\n",
    "!mkdir -p {path_output}\n",
    "_ = joblib.dump(log_probs_trace, f'{path_output}/log_probs_trace.joblib')\n",
    "_ = joblib.dump(class_counts_trace, f'{path_output}/class_counts_trace.joblib')\n",
    "_ = joblib.dump(samples, f'{path_output}/samples.joblib')\n",
    "_ = joblib.dump(x_word_type_posterior_probs, f'{path_output}/x_word_type_posterior_probs.joblib')\n",
    "_ = joblib.dump(sampler, f'{path_output}/sampler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8be29-eb0b-474c-9f52-57e21a6704e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-bayesian_pos_tagger",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
