{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ae86ad-ce52-485f-9f53-6faf885cc410",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9072d-f0ec-4c56-b816-d9b25d6b9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import unicodedata\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import joblib\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c1783-8d07-4411-a993-49f4a61df55b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d0eb9-d422-47d1-837a-1f2e6a5b620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    # text = clean(text.lower())\n",
    "    rx = re.compile(r\"\\b\\p{L}[\\p{L}\\p{M}\\p{N}'â€™-]*\\b\", re.UNICODE)\n",
    "    return rx.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af91b8f-2447-4800-bcd4-1476f43e63e0",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0425d608-b61a-4762-95da-46bd4c7f38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "\n",
    "for f in glob('../data/raw/*.txt'):\n",
    "    for l in open(f, 'r').readlines():\n",
    "        counter.update(tokenize(l))\n",
    "\n",
    "n_most_common_wordtypes = 100\n",
    "n_features = n_most_common_wordtypes + 2\n",
    "most_common_wordtypes = list(map(itemgetter(0), counter.most_common(n_most_common_wordtypes))) + ['<BOS>', '<EOS>']\n",
    "feature_to_ix = dict(zip(most_common_wordtypes, range(len(most_common_wordtypes))))\n",
    "wordtype_to_ix = dict(zip(counter.keys(), range(len(counter.keys()))))\n",
    "ix_to_wordtype = dict(zip(wordtype_to_ix.values(), wordtype_to_ix.keys()))\n",
    "n_wordtypes = len(wordtype_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35466e-b7d3-4699-944c-022173866333",
   "metadata": {},
   "source": [
    "## Featurise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e40895-05a7-4915-8843-29b1799da219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra dimension to count ignored words. to be removed later.\n",
    "x_wordtype_counts_left = np.zeros((n_wordtypes, n_features + 1)).astype(int)\n",
    "x_wordtype_counts_right = np.zeros((n_wordtypes, n_features + 1)).astype(int)\n",
    "\n",
    "for f in glob('../data/raw/*.txt'):\n",
    "    for l in open(f, 'r').readlines():\n",
    "        tokens = ['<BOS>'] + tokenize(l) + ['<EOS>']\n",
    "        for ix in range(1, len(tokens) - 1):\n",
    "            x_wordtype_counts_left[wordtype_to_ix[tokens[ix]], feature_to_ix.get(tokens[ix-1], -1)] += 1\n",
    "            x_wordtype_counts_right[wordtype_to_ix[tokens[ix]], feature_to_ix.get(tokens[ix+1], -1)] += 1\n",
    "            # print(words[ix-1], words[ix], words[ix+1])\n",
    "\n",
    "# remove ignored words\n",
    "x_wordtype_counts_left = x_wordtype_counts_left[:, :n_features]  # M (n_wordtypes) x F (n_features)\n",
    "x_wordtype_counts_right = x_wordtype_counts_right[:, :n_features]  # M (n_wordtypes) x F (n_features)\n",
    "\n",
    "# x_wordtype_counts_sum_left = x_wordtype_counts_left.sum(axis=1)  # M (n_wordtypes)\n",
    "# x_wordtype_counts_sum_right = x_wordtype_counts_right.sum(axis=1)  # M (n_wordtypes)\n",
    "\n",
    "assert x_wordtype_counts_left.shape == x_wordtype_counts_right.shape == (n_wordtypes, n_features)\n",
    "assert x_wordtype_counts_right.shape == x_wordtype_counts_right.shape == (n_wordtypes, n_features)\n",
    "# assert x_wordtype_counts_sum_left.shape == (n_wordtypes,)\n",
    "# assert x_wordtype_counts_sum_right.shape == (n_wordtypes,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65a94f-f0b1-475d-934a-abff2241abd9",
   "metadata": {},
   "source": [
    "# Export artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60aa3542-b2c4-47cc-a8b2-00c06ab19166",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = 'outputs/preprocess'\n",
    "!mkdir -p {path_output}\n",
    "_ = joblib.dump(wordtype_to_ix, f'{path_output}/wordtype_to_ix.joblib')\n",
    "_ = joblib.dump(ix_to_wordtype, f'{path_output}/ix_to_wordtype.joblib')\n",
    "_ = joblib.dump(counter, f'{path_output}/counter.joblib')\n",
    "_ = joblib.dump(x_wordtype_counts_left, f'{path_output}/x_wordtype_counts_left.joblib')\n",
    "_ = joblib.dump(x_wordtype_counts_right, f'{path_output}/x_wordtype_counts_right.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78acbde-09e0-4547-8fb7-fe7f52b14f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-bayesian_pos_tagger",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "bayesian_pos_tagger",
   "language": "python",
   "name": "conda-base-bayesian_pos_tagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
