# A Bayesian Mixture Model for Part-of-Speech unsupervised induction using Gibbs sampling.
![](https://img.shields.io/badge/version-v1.0.0-brightgreen.svg)
[![Python 3.10](https://img.shields.io/badge/python-3.10-brightgreen.svg)](https://www.python.org/downloads/release/python-3/)


---

## ‚ú® What‚Äôs inside

Unsupervised Part-of-Speech (POS) induction using a Bayesian Mixture Model trained via Gibbs sampling. The goal is to learn POS categories directly from raw text without labeled tags, applied to the Mapudungun language.

- **Bayesian Mixture Model** for Part-of-Speech induction  
- **Collapsed Gibbs sampling** for efficient inference  
- **Notebooks** for exploration and diagnostics  
- **Resources** with references
- **Packaging** with `pyproject.toml` / `setup.cfg` for editable installs

---

# üß© Why This POS Tagger Matters

This Bayesian Mixture Model POS Tagger learns syntactic categories directly from raw text, without requiring any annotated training data.
That‚Äôs essential for low-resource languages, where gold-standard POS labels simply don‚Äôt exist.

It replaces expensive manual annotation with statistical inference.

## üåç What Is a Low-Resource Language?

A low-resource language is one that lacks large annotated datasets, digital corpora, or linguistic tools (such as parsers, taggers, or dictionaries) that are typically available for major world languages like English, Spanish, or Chinese.

In practice, this means:

- There are few or no labeled corpora for supervised learning.
- There are limited digital texts and linguistic resources (grammars, lexicons).

## Data

This tagger is designed to work with the [**Mapudungun Corpus**](https://github.com/lqrz/mapudungun-corpus), a companion dataset repository that provides preprocessed text data in Mapudungun for linguistic modeling and evaluation.

--- 

# Generative Model

![Plate diagram](docs/plate_diagram.png)

## üîπ Random Variables

| Symbol | Meaning |
|:-------:|:---------|
| Œ∏ | Mixing proportions over syntactic classes (class priors) |
| z‚±º | Latent class assigned to word type *j* |
| œÜ·µ¢ | Multinomial distribution over features for class *i* |
| f‚±º‚Çñ | Observed feature for the *k*-th token of word type *j* |
| Œ±, Œ≤ | Dirichlet hyperparameters controlling sparsity and smoothing |

## üîπ Generative Story

1. **Latent class priors**  
   Draw a class prior distribution:  Œ∏ | Œ±  ‚àº Dirichlet(Œ±)

2. **For each syntactic class i = 1...Z:**  
   Draw a multinomial distribution over features:  œÜ·µ¢ | Œ≤ ‚àº Dirichlet(Œ≤)

3. **For each word type j = 1...M:**
   - Choose its latent class:  z‚±º | Œ∏ ‚àº Multinomial(Œ∏)
   - For each of its n‚±º observe token features:  f‚±º‚Çñ | œÜ_z‚±º ‚àº Multinomial(œÜ_z‚±º)

---

## üìö Acknowledgements

* [_Christodoulopoulos, et al_](resources/a_bayesian_mixture_model_for_pos_induction_using_multiple_features.pdf): "A Bayesian Mixture Model for Part-of-Speech Induction Using Multiple Features".
